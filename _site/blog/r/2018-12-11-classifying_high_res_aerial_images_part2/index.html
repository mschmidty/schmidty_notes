<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classifying High Resolution Aerial Imagery - Part 2</title>
    <meta name="description" content="">
    <link rel="stylesheet" href="/css/main.css">
  </head>
  <body>
    <header>
      
    </header>

    <main>
      <article class="post-content">
  <header class="post-header">
    <h1 class="post-title">Classifying High Resolution Aerial Imagery - Part 2</h1>
    <div class="post-meta">
      <time datetime="Invalid DateTime" class="post-time">December 01, 2018</time>
      <span>| tags:</span>
      <ul class="tags-list post-tag-list">
      
        
        <li><a href="/tags/r/" class="tag-list-tag">R</a></li>
      
        
        <li><a href="/tags/gis/" class="tag-list-tag">GIS</a></li>
      
        
        <li><a href="/tags/r/" class="tag-list-tag">R</a></li>
      
        
        <li><a href="/tags/remote-sensing/" class="tag-list-tag">Remote Sensing</a></li>
      
      </ul>
    </div>
  </header>
  <p>I have been attempting to use random forests to classify high resolution aerial imagery. <a href="r/2018-11-20-classifying_high_res_aerial_images">Part one</a> of this post series was my first attempt. The aerial imagery dataset that I am working on is made up of many ortho tiles that I need to classify into vegetation categories.  The first attempt was to classify vegetation on one tile.  This note documents classifying vegetation across tiles.</p>
<p>See the r script below.</p>
<h2>Notes</h2>
<h3>Set Up</h3>
<p>The training polygon shapefile consists of 8 classes (it should be 7  but I accidentally used two names for bare ground: &quot;BG_Soil&quot; and &quot;BG_Rock&quot;).</p>
<h3>The Results</h3>
<p>The out of bag (OOB) estimate of error rate was 1.85% which is pretty good.  However, the training dataset has very imbalanced classes.</p>
<table>
<thead>
<tr>
<th>Class</th>
<th>Percent Cover</th>
</tr>
</thead>
<tbody>
<tr>
<td>BG_Rock</td>
<td>25.6</td>
</tr>
<tr>
<td>BG_Soil</td>
<td>24.7</td>
</tr>
<tr>
<td>Black_Sage</td>
<td>1.7</td>
</tr>
<tr>
<td>Grass</td>
<td>4.1</td>
</tr>
<tr>
<td>Other_Shrub</td>
<td>1.5</td>
</tr>
<tr>
<td>Other_Veg</td>
<td>15.2</td>
</tr>
<tr>
<td>PJ</td>
<td>24.2</td>
</tr>
<tr>
<td>Sage</td>
<td>2.9</td>
</tr>
</tbody>
</table>
<p>Classes were very imbalanced in the training dataset.  Both &quot;BG_Soil&quot; and &quot;BG_Rock&quot; should be combined making soil account for over 50% of the area.
{: .caption}</p>
<p>You can see the class imbalance in the confusion matrix as well.</p>
<table>
<thead>
<tr>
<th></th>
<th>BG_Rock</th>
<th>BG_Soil</th>
<th>Black_Sage</th>
<th>Grass</th>
<th>Other_Shrub</th>
<th>Other_Veg</th>
<th>PJ</th>
<th>Sage</th>
<th>class.error</th>
</tr>
</thead>
<tbody>
<tr>
<td>BG_Rock</td>
<td>148953</td>
<td>145</td>
<td>2</td>
<td>24</td>
<td>12</td>
<td>3</td>
<td>1</td>
<td>7</td>
<td>0.0013007301521318348</td>
</tr>
<tr>
<td>BG_Soil</td>
<td>148</td>
<td>141975</td>
<td>206</td>
<td>472</td>
<td>768</td>
<td>40</td>
<td>51</td>
<td>289</td>
<td>0.01371319008815619</td>
</tr>
<tr>
<td>Black_Sage</td>
<td>8</td>
<td>244</td>
<td>8283</td>
<td>254</td>
<td>145</td>
<td>18</td>
<td>67</td>
<td>863</td>
<td>0.1618093503339405</td>
</tr>
<tr>
<td>Grass</td>
<td>25</td>
<td>174</td>
<td>155</td>
<td>23514</td>
<td>11</td>
<td>14</td>
<td>1</td>
<td>32</td>
<td>0.0172197609295327</td>
</tr>
<tr>
<td>Other_Shrub</td>
<td>3</td>
<td>531</td>
<td>61</td>
<td>5</td>
<td>7993</td>
<td>4</td>
<td>7</td>
<td>1</td>
<td>0.07112144102266127</td>
</tr>
<tr>
<td>Other_Veg</td>
<td>1</td>
<td>137</td>
<td>34</td>
<td>87</td>
<td>24</td>
<td>86021</td>
<td>2195</td>
<td>9</td>
<td>0.028099154878654997</td>
</tr>
<tr>
<td>PJ</td>
<td>0</td>
<td>78</td>
<td>203</td>
<td>8</td>
<td>24</td>
<td>688</td>
<td>139218</td>
<td>708</td>
<td>0.012126845813790088</td>
</tr>
<tr>
<td>Sage</td>
<td>0</td>
<td>247</td>
<td>1091</td>
<td>108</td>
<td>9</td>
<td>4</td>
<td>318</td>
<td>15315</td>
<td>0.10396676807863325</td>
</tr>
</tbody>
</table>
<p>The most difficult divisions for the random forests was between &quot;Black_Sage&quot; and &quot;Sage&quot;.  Which makes a lot of sense.  About 6.3% of the time Sage was classified as Black_Sage, out of a total error of 10%, and about 8.7% of the time &quot;Black_Sage&quot; was classified as Sage, out of a total error of 16%.</p>
<h3>Making Improvements (Iterate or Die)</h3>
<p><strong>Class Size:</strong> Obviously, I need to improve the class balance.  I think the best way to do this is to both draw more training polygons in those classes with less (black_sage, sage and other_shrub) coverage and two add a balancing step in the script. Which brings me to <code>sampsize()</code>.</p>
<p><strong>Sample Size</strong> I also need to take advantage of randomForest's ability to manage sample size with <code>sampsize()</code>.  I have had a hard time automating the sample size to be used in the function so that I don't have to find the smallest class and then manually put in that number as many times as there are classes. For instance if I had five classes and the minimum class size was 500 pixels I would need to set sampsize to be <code>sampsize(500,500,500,500,500)</code>.  Or I could sample the combined training set by randomly sampling pixels before I run random forests.</p>
<p><strong>Segmentation</strong> - I still think segmentation has a role to play in this classification process, but my first attempt at segmentation was a complete failure.  I need to find a tutorial on classification before I can make any improvement.</p>
<p><strong>Adding Data</strong> - Using only five variables, red, blue, green, alpha, and ndvi, seems like very little data to classify imagery. There are a few datasets that I am considering adding:</p>
<ul>
<li>Soils data</li>
<li>Elevation type data (slope, flow direction, terrain roughness, etc. )</li>
<li>Landfire data, which is essentially landsat data that has been classified.</li>
</ul>
<p>But if this process is going to scale to the rest of the state, the datasources that I use need to be available to everyone.</p>
<h2>Scripts</h2>
<pre><code class="language-r">
## Load libraries
require(dplyr)
require(stringr)
require(raster)
require(rgdal)
require(rgeos)
require(randomForest)



## Function to work subset training dataset of one tile
# - inputs (all are automatically created by build_model):
# -- tile_name: is the name of the tile from a single iteration in the list
# -- folder_of_tiles: is the directory that all of the tiles are stored.
# -- file_path_to_shape: the path to the shapefile that will classify.
# - steps:
# -- RASTER STEPS
# -- read raster brick
# -- change names to b1 to b4 for each band
# -- create an NDVI layer
# -- combine NDVI to four Bands
# -- renmae each band
# -- SHAPEFILE STEPS
# -- split file path name
# -- read shapefile with split filepath
# -- Don't know what the layer paste is all about, I think I can take it out.
# -- convert Class, the supervised classes for training, to numeric
# -- clip the shapefile to the extent of the loaded tile.
# -- rasterized the clipped shapefile with values being class.
# -- rename heading to class
# -- remove temporary variables
# -- MORE RASTER STEPS
# -- clip the raster to the shapefile extent
# -- add training raster (converted shapefile) to the ortho raster data.
# -- convert raster to values (table)
# -- convert values to data.frame, the above step may be redundant.
# -- filter na's, because when you clip a raster to a geometry, the pixels outside of the geometry are still there but they have values of NA.  This step is the most memory intensive step in the whole process.
# -- save the RDS to be modeled in random forests. There is definitely a better way to do this.....
# -- rm data_to_be_modeled otherwize each iteration will fail on the remove NA step.


subset_training_data_from_tile&lt;- function( tile_name, folder_of_tiles, file_path_to_shape){


  ## read raster make ndvi layer and add it to the brick.
  raster&lt;-brick(paste0(paste(folder_of_tiles, tile_name ,sep=&quot;/&quot;), &quot;.tif&quot; ))
  names(raster)&lt;-c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;) ## Rename Bands for ease of use
  ndvi&lt;-overlay(raster$b4, raster$b3, fun=function(x,y){(x-y)/(x+y)})
  cov&lt;-addLayer(raster,ndvi) ## Merge NDVI and the tile imagery
  names(cov)&lt;-c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;b4&quot;, &quot;ndvi&quot;)

  ## parse and read shapefile
  temp&lt;-stringr::str_split(file_path_to_shape, &quot;/&quot;)
  temp2&lt;-readOGR(dsn=paste(head(temp[[1]], -1) , collapse = &quot;/&quot;), layer=paste(tail(temp[[1]], 1))) ## Don't remember what this does?
  temp2@data$code&lt;-as.numeric(temp2@data$Class)
  shape&lt;-crop(temp2, extent(cov))%&gt;%
    rasterize(ndvi, field=&quot;code&quot;)  
  names(shape)&lt;-&quot;class&quot;

  rm(ndvi, temp, temp2, raster)

  data_to_be_modeled&lt;-crop(cov, extent(shape))%&gt;%
    mask(shape)%&gt;%
    addLayer(shape)%&gt;%
    getValues()%&gt;%
    as.data.frame()%&gt;%
    filter(!is.na(class))

  saveRDS(data_to_be_modeled, paste0(&quot;training_datasets/&quot;, tile_name, &quot;training_data.rds&quot;))

  rm(data_to_be_modeled)
}


## Run random forest on combined training dataset
# - reads the folder with tiles
# -- takes all file names and stores them in a list to be transfered to the `subset_training_data_from_tile` function which
# - reads the shapefile
# -- adds the shapefile to `subset_training_data_from_tile`
# - tile names are looped over and sent to `subset_training_data_from_tile` individually with the shapefile.
# - all individually returned tile training datasets are combined together with rbind.fill
# - combined dataset, is run through random forests.
build_model&lt;-function(folder_of_tiles, file_path_to_shape){
  t&lt;-base::list.files(folder_of_tiles)%&gt;%
    tools::file_path_sans_ext()%&gt;%
    tools::file_path_sans_ext()%&gt;%
    base::unique() ## May want to change this to just a vector and use this for just apply

  t2&lt;-folder_of_tiles
  t3&lt;-file_path_to_shape

  lapply(t, subset_training_data_from_tile, t2, t3)

  t4&lt;-base::list.files(&quot;training_datasets&quot;)

  t5&lt;-lapply(t4, function(x)readRDS(paste0(&quot;training_datasets/&quot;, x)))
  t6 &lt;- do.call(plyr::rbind.fill, t5)

  head(t6)

  rm(t5)

  fit&lt;-randomForest(as.factor(class)~.,
                    data=t6,
                    importance=TRUE,
                    ntree=500, norm.votes = FALSE
  )
  rm(t6)
  return(fit)
}


## Run the function `build_model`
set.seed(420)
start_time &lt;- Sys.time()

test_model&lt;-build_model(&quot;test_tiles&quot;, &quot;test_shape/training_polygons12N_12102018&quot;)

end_time &lt;- Sys.time()

end_time - start_time
</code></pre>

</article>

    </main>

    <footer></footer>

    <!-- Current page: /blog/r/2018-12-11-classifying_high_res_aerial_images_part2/ -->
  </body>
</html>